{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of DT2119_project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VVu1WXfq2Q1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKMXTmDiTqiB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from numpy import newaxis\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n",
        "\n",
        "with open('/content/gdrive/My Drive/train_xspeech.npy', 'rb') as f:\n",
        "            train_x = np.load(f)\n",
        "            y = np.load(f)\n",
        "\n",
        "shape_=np.shape(train_x)\n",
        "x_train=train_x[:,newaxis,:]\n",
        "tf.reshape(x_train,(shape_[0],1, shape_[1]) )\n",
        "x = x_train "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxdrqhW2H7R7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DualStudent(tf.keras.Model):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(DualStudent, self).__init__()\n",
        "        self.nr_of_units=768\n",
        "        self.nr_of_layers=5\n",
        "        self.nr_of_classes=61\n",
        "        self.student_1={}\n",
        "        self.student_2={}\n",
        "        for student in [self.student_1]:#, self.student_2]:\n",
        "            for i in range(self.nr_of_layers-1):\n",
        "                student[\"layer\"+str(i+1)]=tf.keras.layers.LSTM(units=self.nr_of_units, return_sequences=True)  \n",
        "            student[\"layer\"+str(5)]=tf.keras.layers.LSTM(units=self.nr_of_units, return_sequences=False)\n",
        "            student[\"layer\"+str(6)]=tf.keras.layers.Dense(units=self.nr_of_classes)\n",
        "\n",
        "\n",
        "    def call(self,  inputs):\n",
        "        x1=self.student_1[\"layer\"+str(1)](inputs)\n",
        "        #x2=self.student_2[\"layer\"+str(1)](inputs)\n",
        "\n",
        "        for i in range(self.nr_of_layers):\n",
        "            x1=self.student_1[\"layer\"+str(i+2)](x1)\n",
        "            #x2=self.student_2[\"layer\"+str(1+2)](x2)\n",
        "        return x1 #,x2\n",
        "\n",
        "\n",
        "    def train_step(self, data):\n",
        "        x, y = data\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = self(x, training=True)  # Forward pass\n",
        "            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
        "\n",
        "        trainable_vars = self.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "        self.compiled_metrics.update_state(y, y_pred)\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "model = DualStudent()\n",
        "model.compile(optimizer=\"SGD\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(x, y, epochs=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3zLCQm_vfNQ",
        "colab_type": "code",
        "colab": {},
        "cellView": "code"
      },
      "source": [
        "#@title Default title text\n",
        "#Functional API\n",
        "\n",
        "class DualStudent():\n",
        "\n",
        "    def __init__(self, nr_of_units=768, nr_of_layers=6, nr_of_classes=61, student_version=\"Mono_directional\",show_summary=True, epsilon=0.016395):#95):\n",
        "        self.nr_of_units=nr_of_units\n",
        "        self.nr_of_layers=nr_of_layers\n",
        "        self.nr_of_classes=nr_of_classes\n",
        "        self.student_version=student_version\n",
        "        self.x=None\n",
        "        self.y=None\n",
        "        self.lambda_1=1\n",
        "        self.lambda_2=10000\n",
        "        self.epsilon=epsilon\n",
        "        self.cce = tf.keras.losses.CategoricalCrossentropy()\n",
        "        self.mse = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "        self.get_data()\n",
        "        self.show_summary=show_summary\n",
        "        if self.student_version==\"Mono_directional\":\n",
        "            self.student1=self.get_model(\"student1\")\n",
        "            self.student2=self.get_model(\"student2\")\n",
        "\n",
        "        elif self.student_version==\"Imbalanced\":\n",
        "            self.student1=self.get_model(\"student1\")\n",
        "            self.student2=self.get_model(\"student2\", lstm_version=\"Bi_directional\")\n",
        "\n",
        "        else:\n",
        "            self.student1=self.get_model(\"student1\" , lstm_version=\"Bi_directional\" )\n",
        "            self.student2=self.get_model(\"student2\", lstm_version=\"Bi_directional\" )\n",
        "        \n",
        "        self.models={\"student1\":self.student1,\"student2\":self.student2}\n",
        "\n",
        "    def get_data(self):\n",
        "        with open('/content/gdrive/My Drive/train_xspeech.npy', 'rb') as f:\n",
        "            train_x = np.load(f)\n",
        "            self.y = np.load(f)\n",
        "\n",
        "        shape_=np.shape(train_x)\n",
        "        x_train=train_x[:,newaxis,:]\n",
        "        tf.reshape(x_train,(shape_[0],1, shape_[1]) )\n",
        "        self.x = x_train \n",
        "\n",
        "    def get_model(self, name_=\"\", lstm_version=\"Mono_directional\"):\n",
        "        inputs = tf.keras.Input(shape=np.shape(self.x)[1:])\n",
        "\n",
        "        if lstm_version==\"Bi_directional\":\n",
        "            x=tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=self.nr_of_units, return_sequences=True))(inputs) \n",
        "            for i in range(self.nr_of_layers-3):\n",
        "                x=tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=self.nr_of_units, return_sequences=True))(x)  \n",
        "            x=tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=self.nr_of_units, return_sequences=False))(x)  \n",
        "            outputs=tf.keras.layers.Dense(units=self.nr_of_classes)(x)\n",
        "\n",
        "        else:\n",
        "            x=tf.keras.layers.LSTM(units=self.nr_of_units, return_sequences=True)(inputs) \n",
        "            for i in range(self.nr_of_layers-3):\n",
        "                x=tf.keras.layers.LSTM(units=self.nr_of_units, return_sequences=True)(x)  \n",
        "            x=tf.keras.layers.LSTM(units=self.nr_of_units, return_sequences=False)(x)  \n",
        "            outputs=tf.keras.layers.Dense(units=self.nr_of_classes, activation=\"softmax\" )(x)\n",
        "\n",
        "        model = tf.keras.Model(inputs=inputs, outputs=outputs, name=lstm_version+\"_\"+name_)\n",
        "        if self.show_summary:  \n",
        "            model.summary()\n",
        "            print(\"\\n\\n\")\n",
        "        optimizer=tf.keras.optimizers.SGD(learning_rate=0.01, name='SGD')\n",
        "        model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "        return model\n",
        "    \n",
        "    def train(self, x=None, y=None, nr_epochs=100, batch_size=100, unlabeled_x=None):\n",
        "        if x==None and y==None:\n",
        "            x=self.x\n",
        "            y=self.y\n",
        "\n",
        "        self.epochs=nr_epochs\n",
        "        losses={}\n",
        "        stable_samples={}\n",
        "        \n",
        "        for epoch in range(1,self.epochs+1):\n",
        "            model=\"student1\"\n",
        "            Y_pred=self.models[model].predict(x)\n",
        "            print(\"accuracy\",tf.reduce_mean(tf.keras.metrics.categorical_accuracy(y, Y_pred)))\n",
        "            print(\"epoch\",epoch)\n",
        "            for i in range(int(np.ceil(np.size(x_train,0)/batch_size))):\n",
        "                x_batch=x[i*batch_size:(i+1)*batch_size]\n",
        "                y_true=y[i*batch_size:(i+1)*batch_size]\n",
        "                B_1=x_batch + np.random.random(np.shape(x_batch))*0.1\n",
        "                B_2=x_batch + np.random.random(np.shape(x_batch))*0.1\n",
        "\n",
        "                #change this when we get real data\n",
        "                unlabeled_x=x[0:100]\n",
        "                noisy_augmentation = unlabeled_x + np.random.random(np.shape(unlabeled_x))*0.1\n",
        "                \n",
        "                # prints out the accuracy of the first batch every epoch for a choosen model\n",
        "                \n",
        "                \n",
        "                with tf.GradientTape(persistent=True) as tape:\n",
        "                    for model in self.models:\n",
        "                    \n",
        "                        # Calculate L_cls on labeled samples\n",
        "                        y_pred=self.models[model](x_batch)\n",
        "                        loss_cls =  self.cce(y_true, y_pred)\n",
        "\n",
        "                        # Calculate L_con by Eq. 1 between B1 and B2\n",
        "                        y_B_1=self.models[model](B_1)\n",
        "                        y_B_2=self.models[model](B_2)\n",
        "                        loss_con= self.lambda_1 * self.mse(y_B_1, y_B_2)\n",
        "                        losses[model+\"_loss\"] = loss_cls + self.lambda_1 * loss_con\n",
        "                        \n",
        "                        # Determine whether x is stable by Eq. 3\n",
        "                        U_pred=self.models[model](unlabeled_x)\n",
        "                        noisy_pred=self.models[model](noisy_augmentation)\n",
        "                        \n",
        "                        P_i=tf.argmax(U_pred, axis=1)\n",
        "                        P_j=tf.argmax(noisy_pred, axis=1)   \n",
        "                        M_i=tf.math.reduce_max(U_pred, axis=1) \n",
        "                        M_j=tf.math.reduce_max(noisy_pred, axis=1)  \n",
        "                        \n",
        "                        stable_samples[model]=tf.where(P_i==P_j,1,0)*tf.where(M_i>self.epsilon,1,0)*tf.where(M_j>self.epsilon,1,0)  \n",
        "                        stable_samples[model+\"_pred\"]=U_pred \n",
        "                        stable_samples[model+\"_noise\"]=noisy_pred\n",
        "\n",
        "\n",
        "                    # R_1, R_2, R_i, R_j and R_12 does not mean the same thing as in the paper\n",
        "                    R_1=tf.where(stable_samples[\"student1\"]-stable_samples[\"student2\"]>0,True,False)                \n",
        "                    R_2=tf.where(stable_samples[\"student2\"]-stable_samples[\"student1\"]>0,True,False)\n",
        "                    R_12=tf.where(stable_samples[\"student1\"]+stable_samples[\"student2\"]==2,True,False)\n",
        "                    \n",
        "                    # where both R_1 and R_2 are equal to one (R_12) measure prediction consistancy with Euclidean distance\n",
        "                    epsilon_i=tf.math.reduce_euclidean_norm(stable_samples[\"student1_pred\"][R_12]-stable_samples[\"student1_noise\"][R_12], axis=1)\n",
        "                    epsilon_j=tf.math.reduce_euclidean_norm(stable_samples[\"student2_pred\"][R_12]-stable_samples[\"student2_noise\"][R_12], axis=1)\n",
        "                    R_i=epsilon_i>epsilon_j\n",
        "                    R_j=epsilon_i<=epsilon_j\n",
        "\n",
        "                    # loss_sta for student 1\n",
        "                    sample1_update1=tf.concat([stable_samples[\"student1_pred\"][R_1], stable_samples[\"student1_pred\"][R_12][R_i]],axis=0)\n",
        "                    sample2_update1=tf.concat([stable_samples[\"student2_pred\"][R_1], stable_samples[\"student2_pred\"][R_12][R_i]],axis=0)\n",
        "                    loss_sta=self.mse(sample1_update1, sample2_update1)\n",
        "                    losses[\"student1_loss\"] = losses[\"student1_loss\"] + self.lambda_2 * loss_sta\n",
        "\n",
        "                    # loss_sta for student 2\n",
        "                    sample1_update2=tf.concat([stable_samples[\"student1_pred\"][R_2], stable_samples[\"student1_pred\"][R_12][R_j]],axis=0)\n",
        "                    sample2_update2=tf.concat([stable_samples[\"student2_pred\"][R_2], stable_samples[\"student2_pred\"][R_12][R_j]],axis=0)\n",
        "                    loss_sta=self.mse(sample1_update2, sample2_update2)\n",
        "                    losses[\"student2_loss\"] = losses[\"student2_loss\"] + self.lambda_2 * loss_sta\n",
        "\n",
        "                # update the model parameters \n",
        "                for model in self.models:\n",
        "                    trainable_vars = self.models[model].trainable_variables\n",
        "                    gradients = tape.gradient(losses[model+\"_loss\"], trainable_vars)\n",
        "                    self.models[model].optimizer.apply_gradients(zip(gradients, trainable_vars))                  \n",
        "\n",
        "        return \n",
        "\n",
        "models={}\n",
        "for version_ in [\"Mono_directional\", \"Imbalanced\", \"Bi_directional\"]:\n",
        "    models[version_]=DualStudent(student_version=version_)\n",
        "    print(\"\\n\\n\\n\")\n",
        "\n",
        "models[\"Mono_directional\"].train(nr_epochs=5)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}